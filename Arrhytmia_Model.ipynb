{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw1wniLI92Tx",
        "outputId": "f159e52a-0e51-4dc5-acf2-f675d4763aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.11.15)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2025.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.14.1)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.20.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets, pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyWavelets-1.8.0 pandas-2.2.3 wfdb-4.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb PyWavelets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import wfdb\n",
        "import pywt\n",
        "from scipy.signal import medfilt\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "from sklearn.preprocessing  import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm           import SVC\n",
        "from sklearn.metrics       import classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "HSDj2-V4-CP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will fetch all 48 records into \"./mitbih-database\"\n",
        "if not os.path.isdir('mitbih-database'):\n",
        "    wfdb.dl_database('mitdb', 'mitbih-database')\n"
      ],
      "metadata": {
        "id": "7viwUgYh-GLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FS = 360    # sampling rate\n",
        "W  = 180    # window length (samples)\n",
        "H  = W // 2\n",
        "\n",
        "# Inter-patient split IDs from the paper\n",
        "DS1 = [\"101\",\"106\",\"108\",\"109\",\"112\",\"114\",\"115\",\"116\",\"118\",\"119\",\"122\",\"124\",\n",
        "       \"201\",\"203\",\"205\",\"207\",\"208\",\"209\",\"215\",\"220\",\"223\",\"230\"]\n",
        "DS2 = [\"100\",\"103\",\"105\",\"111\",\"113\",\"117\",\"121\",\"123\",\"200\",\"202\",\"210\",\"212\",\n",
        "       \"213\",\"219\",\"221\",\"222\",\"228\",\"231\",\"232\",\"233\",\"234\"]\n",
        "\n",
        "# small prototype splits\n",
        "DS1_small = DS1[:5]   # first 5 records for train\n",
        "DS2_small = DS2[:5]   # first 5 records for test\n",
        "\n"
      ],
      "metadata": {
        "id": "51YQUMvX-PHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_record(rec_id):\n",
        "    rec = wfdb.rdrecord(f'mitbih-database/{rec_id}')\n",
        "    ann = wfdb.rdann(   f'mitbih-database/{rec_id}', 'atr')\n",
        "    sig = rec.p_signal[:,0]  # Lead II\n",
        "\n",
        "    # Baseline removal: 200ms & 600ms median filters\n",
        "    b1  = medfilt(sig, kernel_size=int(0.2*FS)|1)\n",
        "    b2  = medfilt(b1, kernel_size=int(0.6*FS)|1)\n",
        "    sig = sig - b2\n",
        "\n",
        "    return sig, ann.sample, ann.symbol\n",
        "\n",
        "train_raw = [load_record(r) for r in DS1_small]\n",
        "test_raw  = [load_record(r) for r in DS2_small]\n"
      ],
      "metadata": {
        "id": "wRbFH_XE-Saq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map MIT-BIH symbol â†’ AAMI class index (0:N, 1:SVEB, 2:VEB, 3:Fusion)\n",
        "def sym2cls(s):\n",
        "    if   s in ['N','L','R']:           return 0\n",
        "    elif s in ['A','a','J','S','e','j']: return 1\n",
        "    elif s in ['V','E']:               return 2\n",
        "    elif s == 'F':                     return 3\n",
        "    else:                              return None\n",
        "\n",
        "# R-R feature calculator (pre,post,local(10),global + normalize)\n",
        "def rr_feats(peaks):\n",
        "    rr = np.diff(peaks)/FS\n",
        "    pre  = np.insert(rr, 0, rr[0])\n",
        "    post = np.append(rr, rr[-1])\n",
        "    local  = pd.Series(pre).rolling(10, min_periods=1).mean().values\n",
        "    global_ = np.full_like(pre, pre.mean())\n",
        "    feats = np.vstack([pre, post, local, global_,\n",
        "                       pre/pre.mean(), post/post.mean(),\n",
        "                       local/local.mean(), global_/global_.mean()]).T\n",
        "    return feats  # shape = (len(peaks), 8)\n",
        "\n",
        "def segment_record(sig, peaks, symbols):\n",
        "    beats, labs, idxs = [], [], []\n",
        "    for i,(p,s) in enumerate(zip(peaks, symbols)):\n",
        "        cls = sym2cls(s)\n",
        "        if cls is None or p-H < 0 or p+H > len(sig):\n",
        "            continue\n",
        "        beats.append(sig[p-H : p+H])\n",
        "        labs .append(cls)\n",
        "        idxs.append(i)\n",
        "    return np.array(beats), np.array(labs), np.array(idxs)\n",
        "\n",
        "# Process train\n",
        "Xb_tr_list, y_tr_list, rr_tr_list = [], [], []\n",
        "for sig, peaks, syms in train_raw:\n",
        "    b, l, idx = segment_record(sig, peaks, syms)\n",
        "    Xb_tr_list.append(b)\n",
        "    y_tr_list.append(l)\n",
        "    rr_tr_list.append(rr_feats(peaks)[idx])\n",
        "Xb_tr = np.vstack(Xb_tr_list)\n",
        "y_tr  = np.hstack(y_tr_list)\n",
        "X_rr_tr = np.vstack(rr_tr_list)\n",
        "\n",
        "# Process test\n",
        "Xb_te_list, y_te_list, rr_te_list = [], [], []\n",
        "for sig, peaks, syms in test_raw:\n",
        "    b, l, idx = segment_record(sig, peaks, syms)\n",
        "    Xb_te_list.append(b)\n",
        "    y_te_list.append(l)\n",
        "    rr_te_list.append(rr_feats(peaks)[idx])\n",
        "Xb_te = np.vstack(Xb_te_list)\n",
        "y_te  = np.hstack(y_te_list)\n",
        "X_rr_te = np.vstack(rr_te_list)\n"
      ],
      "metadata": {
        "id": "r6lrUV_D-UGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Wavelets (db1 level 3 â†’ ~23 coeffs)\n",
        "def wave_feats(b):\n",
        "    return np.hstack(pywt.wavedec(b, 'db1', level=3))\n",
        "\n",
        "# 6.2 Higher-Order Stats on 5 segments â†’ 10 dims\n",
        "def hos_feats(b):\n",
        "    segs = np.array_split(b, 5)\n",
        "    out = []\n",
        "    for s in segs:\n",
        "        out += [ skew(s), kurtosis(s) ]\n",
        "    return np.array(out)\n",
        "\n",
        "# 6.3 1D-LBP radius=4 â†’ 256-bin histogram\n",
        "def lbp1d(b, r=4):\n",
        "    codes = []\n",
        "    for i in range(r, len(b)-r):\n",
        "        left  = (b[i-r:i] > b[i]).astype(int)\n",
        "        right = (b[i+1:i+r+1] > b[i]).astype(int)\n",
        "        bits  = np.concatenate([left, right])\n",
        "        codes.append(sum(bit<<j for j,bit in enumerate(bits)))\n",
        "    hist, _ = np.histogram(codes, bins=256, range=(0,256))\n",
        "    return hist\n",
        "\n",
        "# 6.4 Morphological distances\n",
        "def morph_feats(b):\n",
        "    center = len(b)//2\n",
        "    i1 = np.argmax(b[:40])\n",
        "    i2 = 75 + np.argmin(b[75:85])\n",
        "    i3 = 95 + np.argmin(b[95:105])\n",
        "    i4 = 150 + np.argmax(b[150:])\n",
        "    return np.array([\n",
        "        np.hypot(i1-center, b[i1]),\n",
        "        np.hypot(i2-center, b[i2]),\n",
        "        np.hypot(i3-center, b[i3]),\n",
        "        np.hypot(i4-center, b[i4]),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "ZH7UuFcf-WM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 â€“ Extract & Scale All Feature Sets (Parallelized)\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scalers = {}\n",
        "\n",
        "# Helper to parallelize vstack of feature-fn over beats\n",
        "def extract_and_stack(feat_fn, beat_array):\n",
        "    feats = Parallel(n_jobs=-1)(\n",
        "        delayed(feat_fn)(b) for b in beat_array\n",
        "    )\n",
        "    return np.vstack(feats)\n",
        "\n",
        "# 7.1 Wavelet features\n",
        "X_wav_tr = extract_and_stack(wave_feats, Xb_tr)\n",
        "X_wav_te = extract_and_stack(wave_feats, Xb_te)\n",
        "scalers['wav'] = StandardScaler().fit(X_wav_tr)\n",
        "X_wav_tr = scalers['wav'].transform(X_wav_tr)\n",
        "X_wav_te = scalers['wav'].transform(X_wav_te)\n",
        "\n",
        "# 7.2 HOS features\n",
        "X_hos_tr = extract_and_stack(hos_feats, Xb_tr)\n",
        "X_hos_te = extract_and_stack(hos_feats, Xb_te)\n",
        "scalers['hos'] = StandardScaler().fit(X_hos_tr)\n",
        "X_hos_tr = scalers['hos'].transform(X_hos_tr)\n",
        "X_hos_te = scalers['hos'].transform(X_hos_te)\n",
        "\n",
        "# 7.3 1D-LBP features\n",
        "X_lbp_tr = extract_and_stack(lbp1d, Xb_tr)\n",
        "X_lbp_te = extract_and_stack(lbp1d, Xb_te)\n",
        "scalers['lbp'] = StandardScaler().fit(X_lbp_tr)\n",
        "X_lbp_tr = scalers['lbp'].transform(X_lbp_tr)\n",
        "X_lbp_te = scalers['lbp'].transform(X_lbp_te)\n",
        "\n",
        "# 7.4 Morphological features\n",
        "X_morph_tr = extract_and_stack(morph_feats, Xb_tr)\n",
        "X_morph_te = extract_and_stack(morph_feats, Xb_te)\n",
        "scalers['morph'] = StandardScaler().fit(X_morph_tr)\n",
        "X_morph_tr = scalers['morph'].transform(X_morph_tr)\n",
        "X_morph_te = scalers['morph'].transform(X_morph_te)\n",
        "\n",
        "# 7.5 R-R Interval features\n",
        "scalers['rr'] = StandardScaler().fit(X_rr_tr)\n",
        "X_rr_tr = scalers['rr'].transform(X_rr_tr)\n",
        "X_rr_te = scalers['rr'].transform(X_rr_te)\n",
        "\n",
        "print(\"Feature extraction + scaling complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt_Kg7VV-YQW",
        "outputId": "4709bcac-d326-4da8-fb34-795ce9db6901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction + scaling complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-clean all features and align y_tr\n",
        "X_clean = {}\n",
        "y_clean = {}\n",
        "\n",
        "# 1. RR: already separate\n",
        "X_clean['rr'] = X_rr_tr\n",
        "y_clean['rr'] = y_tr[:len(X_rr_tr)]\n",
        "\n",
        "# 2. All beat-aligned features\n",
        "X_feature_sources = {\n",
        "    'wav': X_wav_tr,\n",
        "    'hos': X_hos_tr,\n",
        "    'lbp': X_lbp_tr,\n",
        "    'morph': X_morph_tr\n",
        "}\n",
        "\n",
        "for name, X in X_feature_sources.items():\n",
        "    N = min(len(X), len(y_tr))   # ensure no IndexError\n",
        "    X = X[:N]\n",
        "    y = y_tr[:N]\n",
        "\n",
        "    mask = ~np.isnan(X).any(axis=1)\n",
        "    X_clean[name] = X[mask]\n",
        "    y_clean[name] = y[mask]\n",
        "\n",
        "    print(f\"{name.upper()}: removed {np.sum(~mask)} rows with NaNs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lda7OueObRzE",
        "outputId": "526bcacb-efb2-47cf-d24d-98cfb85dd358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WAV: removed 0 rows with NaNs\n",
            "HOS: removed 0 rows with NaNs\n",
            "LBP: removed 0 rows with NaNs\n",
            "MORPH: removed 0 rows with NaNs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_svm(X, y):\n",
        "    gamma = 1.0 / X.shape[1]\n",
        "    gs = GridSearchCV(\n",
        "        SVC(kernel='rbf', probability=True, gamma=gamma),\n",
        "        {'C': [1e-3,1e-2,1e-1,1,10,100]},\n",
        "        cv=3, n_jobs=-1\n",
        "    )\n",
        "    gs.fit(X, y)\n",
        "    return gs.best_estimator_\n",
        "\n",
        "svm_rr    = train_svm(X_rr_tr,    y_tr)\n",
        "svm_wav   = train_svm(X_wav_tr,   y_tr)\n",
        "svm_hos   = train_svm(X_hos_tr,   y_tr)\n",
        "svm_lbp   = train_svm(X_lbp_tr,   y_tr)\n",
        "svm_morph = train_svm(X_morph_tr, y_tr)\n"
      ],
      "metadata": {
        "id": "u6jGc2R2-aBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = np.stack([\n",
        "    svm_rr   .predict_proba(X_rr_te),\n",
        "    svm_wav  .predict_proba(X_wav_te),\n",
        "    svm_hos  .predict_proba(X_hos_te),\n",
        "    svm_lbp  .predict_proba(X_lbp_te),\n",
        "    svm_morph.predict_proba(X_morph_te),\n",
        "], axis=0)  # shape = (5, n_samples, 4)\n",
        "\n",
        "prod    = np.prod(probs, axis=0)       # (n_samples, 4)\n",
        "y_pred  = prod.argmax(axis=1)\n"
      ],
      "metadata": {
        "id": "EjH03fV8-bo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# Automatically detect predicted class indices\n",
        "present_classes = np.unique(np.concatenate([y_te, y_pred]))\n",
        "\n",
        "# Full label names in order (0 = N, 1 = SVEB, 2 = VEB, 3 = Fusion)\n",
        "label_names = ['N', 'SVEB', 'VEB', 'Fusion']\n",
        "used_names = [label_names[i] for i in present_classes]\n",
        "\n",
        "# 1. Classification report (precision/recall/f1)\n",
        "print(\"ğŸ“Š Classification Report:\")\n",
        "print(classification_report(\n",
        "    y_te, y_pred,\n",
        "    labels=present_classes,\n",
        "    target_names=used_names,\n",
        "    zero_division=0  # prevents divide-by-zero warnings\n",
        "))\n",
        "\n",
        "# 2. Confusion matrix\n",
        "print(\"\\nğŸ§¾ Confusion Matrix:\")\n",
        "print(confusion_matrix(\n",
        "    y_te, y_pred,\n",
        "    labels=present_classes\n",
        "))\n",
        "\n",
        "# 3. Accuracy\n",
        "acc = accuracy_score(y_te, y_pred)\n",
        "print(f\"\\nâœ… Overall Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0Amy32vvaI2",
        "outputId": "6256820d-05fc-4ba6-9312-3457c4d68429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N       0.99      1.00      1.00     10756\n",
            "        SVEB       0.00      0.00      0.00        41\n",
            "         VEB       0.79      0.44      0.57        43\n",
            "\n",
            "    accuracy                           0.99     10840\n",
            "   macro avg       0.60      0.48      0.52     10840\n",
            "weighted avg       0.99      0.99      0.99     10840\n",
            "\n",
            "\n",
            "ğŸ§¾ Confusion Matrix:\n",
            "[[10751     0     5]\n",
            " [   41     0     0]\n",
            " [   24     0    19]]\n",
            "\n",
            "âœ… Overall Accuracy: 0.9935\n"
          ]
        }
      ]
    }
  ]
}